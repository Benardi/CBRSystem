---
title: "Case Based Reasoning System for MSRP estimation"
author: "Jos√© Benardi de Souza Nunes"
date: 07/07/2018
output:
  html_notebook:
    toc: yes
    toc_float: yes
  html_document:
    df_print: paged
    toc: yes
    toc_float: yes
---

<br>

# Introduction

<br>

> Employed data, scripts and a brief description can be found at the [original repository](https://github.com/Benardi/CBRSystem). Further references can be found at
the page of [Prof. Ian Watson](https://www.cs.auckland.ac.nz/~ian/CBR/).

```{r message=FALSE, warning=FALSE}
library(FNN)
library(here)
library(magrittr)
library(tidyverse)

source(here::here("code/calc_KNN_error.R"))

theme_set(theme_bw())
```

<br>

***

<br>

# Data Overview

<br>

Data has the following attributes:

- `Make`: Make of the car;
- `Model`: Model of the car;
- `Year`: Manufacturing Date;
- `Engine.Fuel.Type`: Kind of fuel the engine runs on;
- `Engine.HP`: Engine HorsePower;
- `Engine.Cylinders`: Number of cylinders in the engine;
- `Transmission.Type`: Type of car transmission;
- `Driven_Wheels`: Wheels added;
- `Number.of.Doors`: Number of doors;
- `Vehicle.Size`: Vehycle size;
- `Vehicle.Style`: Vehycle style;
- `highway.MPG`: Miles per gallon on road;
- `city.mpg`: Miles per gallon on city;
- `Popularity`: Car popularity;
- `MSRP`: Manufacturer's Suggested Retail Price and target variable.

<br>

#### Loading Data

```{r}
read_csv(here::here("data/data.csv"),
         progress = FALSE, 
         col_types =
           cols(
            Make = col_character(),
            Model = col_character(),
            Year = col_integer(),
            `Engine Fuel Type` = col_character(),
            `Engine HP` = col_integer(),
            `Engine Cylinders` = col_integer(),
            `Transmission Type` = col_character(),
            Driven_Wheels = col_character(),
            `Number of Doors` = col_integer(),
            `Market Category` = col_character(),
            `Vehicle Size` = col_character(),
            `Vehicle Style` = col_character(),
            `highway MPG` = col_integer(),
            `city mpg` = col_integer(),
            Popularity = col_integer(),
            MSRP = col_integer()
            )) %>% 
  drop_na() -> car_data 
```

## Dummify Categorical Variables 

```{r}
car_data %>%
  mutate(
    Make = as.numeric(factor(Make)),
    Model = as.numeric(factor(Model)),
    `Engine Fuel Type` = as.numeric(factor(`Engine Fuel Type`)),
    `Transmission Type` = as.numeric(factor(`Transmission Type`)),
    Driven_Wheels = as.numeric(factor(Driven_Wheels)),
    `Market Category` = as.numeric(factor(`Market Category`)),
    `Vehicle Size` = as.numeric(factor(`Vehicle Size`)),
    `Vehicle Style` = as.numeric(factor(`Vehicle Style`)))-> car_data 

car_data %>% 
    glimpse()
```

<br>

## Checking for missing values

```{r}
row.has.na <- apply(car_data, 
                    1,
                    function(x){any(is.na(x))})
noquote(paste('Number of rows with misssing values: ',
            sum(row.has.na)))
```

<br>

## Applying scale to predictor variables

```{r}

num.vars <- sapply(car_data,
                   is.numeric, 
                   simplify=F)

num.vars$MSRP = FALSE
num.vars <- unlist(num.vars)

car_data[num.vars] <- lapply(car_data[num.vars],
                             scale)
car_data %>% 
  sample_n(10)
```

***

<br>

# Validation

<br>

## Split data into training/testing sets 

```{r}
set.seed(101)

## Adding surrogate key to dataframe
car_data$id <- 1:nrow(car_data)

car_data %>% 
  dplyr::sample_frac(.8) -> train

dplyr::anti_join(car_data, 
                 train, 
                 by = 'id') -> test
```

## Dissociate predictors from target variable/surrogate key

```{r}
train %>% 
  select(-MSRP,-id) -> train.predictors

train %>% 
  select(MSRP, id) -> train.response

test %>% 
  select(-MSRP,-id) -> test.predictors

test %>% 
  select(MSRP, id) -> test.response
```

***

<br>

# Apply K Nearest Neighbor

<br>

## Calculate accumulated error

```{r}
results <- data.frame(matrix(ncol = 0, nrow = 10))
results$k <- seq(1,10,1)

accum_err <- c()

for(num in results$k) {

  calc_KNN_error(num,
         train.predictors,
         test.predictors,
         train$id,
         train.response,
         test.response) -> err
  
  accum_err <-c(accum_err, err)
}

results$accum_err <- accum_err

results
```

```{r}
results %>%
  ggplot(aes(k,accum_err)) +
  geom_point(size = 3,
             alpha = .6) + 
  geom_line() +
  scale_x_continuous(breaks=seq(1,10,1)) +
  labs(y="Accumulated Error", x= "K Value") +
  ggtitle("Accumulated Error by K value")
```

<br>

## Results

<br>

* A smaller K seems to render less accumulated error.
* At K = 2, we have an unusual spike in terms of accumulated error, which one could impute on a probable overfit. 
* **K = 1** renders the smallest amount of Accumulated Error.

